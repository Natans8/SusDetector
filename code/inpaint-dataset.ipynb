{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from diffusers import FluxKontextInpaintPipeline\n",
    "from nunchaku import NunchakuFluxTransformer2DModelV2\n",
    "from nunchaku.utils import get_precision\n",
    "from tqdm.notebook import tqdm\n",
    "from ultralytics.utils.ops import xywhn2xyxy, xyxy2xywhn\n",
    "from ultralytics.utils.plotting import Annotator"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "prompts = [\n",
    "    \"Add a large black suitcase on the ground\",\n",
    "    \"Add a red backpack on the ground\",\n",
    "    \"Add a big box on the ground\",\n",
    "    \"Add a large sack on the ground\",\n",
    "    \"Add a large hard-shell suitcase on the ground\",\n",
    "    \"Add a battered leather suitcase on the ground\",\n",
    "    \"Add a metallic silver briefcase on the ground\",\n",
    "    \"Add a blue gym bag on the ground\",\n",
    "    \"Add a suspicious duffle bag on the ground\",\n",
    "    \"Add a military tactical backpack on the ground\",\n",
    "    \"Add a bulky hiking backpack on the ground\",\n",
    "    \"Add a dirty canvas bag on the ground\"\n",
    "    \"Add a colorful school bag on the ground\"\n",
    "]"
   ],
   "id": "66d26334c1260e00",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Prepare output directories\n",
    "data_dir = '../data/final'\n",
    "image_dir = f\"{data_dir}/images\"\n",
    "label_dir = f\"{data_dir}/labels\"\n",
    "annotated_dir = f\"{data_dir}/annotated\""
   ],
   "id": "ec29c3edc532424b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Skip to continue previous final\n",
    "if os.path.exists(data_dir):\n",
    "    candidate = data_dir\n",
    "    i = 0\n",
    "    while os.path.exists(candidate):\n",
    "        i += 1\n",
    "        candidate = f\"{data_dir}_{i}\"\n",
    "    data_dir = candidate\n",
    "    image_dir = f\"{data_dir}/images\"\n",
    "    label_dir = f\"{data_dir}/labels\"\n",
    "    annotated_dir = f\"{data_dir}/annotated\"\n",
    "\n",
    "os.makedirs(data_dir, exist_ok=False)\n",
    "os.makedirs(image_dir, exist_ok=False)\n",
    "os.makedirs(label_dir, exist_ok=False)\n",
    "os.makedirs(annotated_dir, exist_ok=False)"
   ],
   "id": "bb58a1a773f33d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_flux_kontext_pipeline() -> FluxKontextInpaintPipeline:\n",
    "    \"\"\"\n",
    "    :return: A FluxKontextInpaintPipeline with Nunchaku FLUX.1-Kontext-Dev transformer\n",
    "    :rtype: FluxKontextInpaintPipeline\n",
    "    \"\"\"\n",
    "    transformer = NunchakuFluxTransformer2DModelV2.from_pretrained(\n",
    "        f\"nunchaku-tech/nunchaku-flux.1-kontext-dev/svdq-{get_precision()}_r32-flux.1-kontext-dev.safetensors\")\n",
    "\n",
    "    transformer.set_attention_backend('sage')\n",
    "\n",
    "    pipe = FluxKontextInpaintPipeline.from_pretrained(\"black-forest-labs/FLUX.1-Kontext-dev\", transformer=transformer,\n",
    "                                                      torch_dtype=torch.bfloat16).to('cuda')\n",
    "\n",
    "    pipe._exclude_from_cpu_offload.append(\"transformer\")\n",
    "    pipe.enable_model_cpu_offload()\n",
    "\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def flux_kontext_inpaint(pipe: FluxKontextInpaintPipeline, img: Image.Image, bbox: np.ndarray,\n",
    "                         prompt: str) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Inpaints an image region defined by bbox using Flux.1 Kontext.\n",
    "    :param pipe: A Flux.1 Kontext inpaint pipeline\n",
    "    :type pipe: FluxKontextInpaintPipeline\n",
    "    :param img: Input image\n",
    "    :type img: Image.Image\n",
    "    :param bbox: Bounding box [x1, y1, x2, y2]\n",
    "    :type bbox: np.ndarray\n",
    "    :param prompt: Inpainting prompt\n",
    "    :type prompt: str\n",
    "    :return: Inpainted image\n",
    "    :rtype: Image.Image\n",
    "    \"\"\"\n",
    "    img_np = np.array(img)\n",
    "    h, w = img_np.shape[:2]\n",
    "\n",
    "    mask = np.zeros((h, w), dtype=np.uint8)\n",
    "    x1, y1, x2, y2 = bbox.astype(int)\n",
    "    mask[y1:y2, x1:x2] = 255  # White rectangle for inpainting area\n",
    "\n",
    "    mask_pil = Image.fromarray(mask)\n",
    "\n",
    "    seed = random.randint(0, 2 ** 31 - 1)\n",
    "    generator = torch.Generator(device='cuda').manual_seed(seed)\n",
    "\n",
    "    inpainted_img = pipe(\n",
    "        prompt=prompt,\n",
    "        image=img,\n",
    "        mask_image=mask_pil,\n",
    "        guidance_scale=2.5,\n",
    "        generator=generator,\n",
    "        strength=1.0).images[0]\n",
    "\n",
    "    return inpainted_img"
   ],
   "id": "b13c8a5982057c67",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def read_yolo_labels(txt_path: str | Path) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Reads YOLO format labels from a text file.\n",
    "    :param txt_path: Path to the YOLO label text file.\n",
    "    :type txt_path: str | Path\n",
    "    :return: Nx4 array of bounding boxes in normalized xywh format.\n",
    "    \"\"\"\n",
    "    txt_path = Path(txt_path)\n",
    "    if not txt_path.exists() or txt_path.stat().st_size == 0:\n",
    "        return np.zeros((0, 5), dtype=np.float32)\n",
    "\n",
    "    rows = []\n",
    "    for line in txt_path.read_text(encoding=\"utf-8\").splitlines():\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        parts = line.split()\n",
    "        if len(parts) != 5:\n",
    "            raise ValueError(f\"Bad label line in {txt_path}: {line}\")\n",
    "        rows.append([float(p) for p in parts])\n",
    "    return np.asarray(rows, dtype=np.float32)[:, 1:]\n",
    "\n",
    "\n",
    "def top_k_by_area_xywh(xywh: np.ndarray, k: int = 3) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Selects the top k bounding boxes by area from an array of xywh boxes.\n",
    "    :param xywh: Array of bounding boxes in xywh format.\n",
    "    :type xywh: np.ndarray\n",
    "    :param k: Number of top boxes to select.\n",
    "    :type k: int\n",
    "    :return: Kx4 array of top k bounding boxes by area.\n",
    "    :rtype: np.ndarray\n",
    "    \"\"\"\n",
    "    xywh = np.asarray(xywh)\n",
    "    if xywh.size == 0:\n",
    "        return xywh.reshape(0, 4)\n",
    "\n",
    "    areas = xywh[:, 2] * xywh[:, 3]\n",
    "    k = min(k, len(xywh))\n",
    "    idx = np.argsort(areas)[::-1][:k]\n",
    "    return xywh[idx]"
   ],
   "id": "dbd0a858492becdd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def box_from_diff(img1, img2, threshold=25, area_threshold=1000) -> tuple | None:\n",
    "    \"\"\"\n",
    "    Extract bounding box around new object in difference image.\n",
    "    :param img1: First image\n",
    "    :type img1: Image.Image\n",
    "    :param img2: Second image\n",
    "    :type img2: Image.Image\n",
    "    :param threshold: Pixel difference threshold\n",
    "    :type threshold: int\n",
    "    :param area_threshold: Minimum area of detected object\n",
    "    :type area_threshold: int\n",
    "    :return: Bounding box (x1, y1, x2, y2) or None if no object detected\n",
    "    :rtype: tuple | None\n",
    "    \"\"\"\n",
    "\n",
    "    diff_image = cv2.absdiff(np.array(img1), np.array(img2))\n",
    "\n",
    "    # Convert to grayscale if needed\n",
    "    if len(diff_image.shape) == 3:\n",
    "        gray = cv2.cvtColor(diff_image, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray = diff_image\n",
    "\n",
    "    # Apply threshold\n",
    "    _, binary = cv2.threshold(gray, threshold, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if not contours:\n",
    "        return None\n",
    "\n",
    "    # Get bounding box of largest contour\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "    if cv2.contourArea(largest_contour) < area_threshold:\n",
    "        return None\n",
    "\n",
    "    x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "\n",
    "    # Convert to xyxy format\n",
    "    bbox = (x, y, x + w, y + h)\n",
    "\n",
    "    return bbox"
   ],
   "id": "3d43d571c568f41b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "pipe = load_flux_kontext_pipeline()",
   "id": "5ce0852fa18d537a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_images = 374\n",
    "failed_removals = 0\n",
    "failed_adds = 0\n",
    "with tqdm(total=num_images, position=1, leave=True) as pbar:\n",
    "    for i in range(num_images):\n",
    "\n",
    "        if os.path.exists(f\"{image_dir}/{i}.png\"):\n",
    "            pbar.update(1)\n",
    "            print(f\"Image {i} already processed, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Load image and label\n",
    "        image = Image.open(f'data/backgrounds/{i}.png').convert(\"RGB\")\n",
    "        labels = read_yolo_labels(f\"data/people-box/labels/{i}.txt\")\n",
    "\n",
    "        # Grow boxes by 5% and select a random person from the top 3 largest\n",
    "        labels[:, 2] *= 1.05\n",
    "        labels[:, 3] *= 1.05\n",
    "        top3 = top_k_by_area_xywh(labels, k=3)\n",
    "        xyxy = xywhn2xyxy(top3, w=1248, h=832)\n",
    "        box = random.choice(xyxy)\n",
    "        x1, y1, x2, y2 = box\n",
    "\n",
    "        # Remove the selected person from the image\n",
    "        remove_people = flux_kontext_inpaint(pipe, image, box, \"Remove all people\")\n",
    "\n",
    "        # Validate person removal\n",
    "        if not box_from_diff(image, remove_people,\n",
    "                             area_threshold=420):  # According to EDA, 423 px is the smallest person box in all the data\n",
    "            # Retry with a different person, if available\n",
    "            if len(xyxy) > 1:\n",
    "                box2 = box\n",
    "                while np.array_equal(box2, box):\n",
    "                    box2 = random.choice(xyxy)\n",
    "                box = box2\n",
    "                remove_people = flux_kontext_inpaint(pipe, image, box, \"Remove all people\")\n",
    "            else:\n",
    "                print(f\"Person removal failed in image {i}, skipping.\")\n",
    "                failed_removals += 1\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "            # Skip if still fails\n",
    "            if not box_from_diff(image, remove_people, area_threshold=420):\n",
    "                print(f\"Person removal failed in image {i}, skipping.\")\n",
    "                failed_removals += 1\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "        # Add an object in place of the removed person\n",
    "        short_box = np.array([x1, y1 + 0.4 * abs(y2 - y1), x2, y2])  # Lower the box by 40%\n",
    "        prompt = random.choice(prompts)\n",
    "        add_object = flux_kontext_inpaint(pipe, remove_people, short_box, prompt)\n",
    "        pbar.refresh()\n",
    "\n",
    "        # Extract bounding box of the added object\n",
    "        object_box = box_from_diff(remove_people, add_object)  # Calculate difference between inpainted images\n",
    "        pbar.refresh()\n",
    "\n",
    "        # Validate the output\n",
    "        if not object_box:\n",
    "            print(f\"No object detected in image {i}, skipping.\")\n",
    "            failed_adds += 1\n",
    "            pbar.update(1)\n",
    "            continue\n",
    "\n",
    "        xywhn = xyxy2xywhn(np.asarray(object_box, dtype=np.float32), w=1248, h=832)\n",
    "        labels = f'0 {xywhn[0]} {xywhn[1]} {xywhn[2]} {xywhn[3]}'  # YOLO format\n",
    "\n",
    "        # Save outputs\n",
    "        add_object.save(f\"{image_dir}/{i}.png\")\n",
    "        with open(f\"{label_dir}/{i}.txt\", 'w') as f:\n",
    "            f.write(labels)\n",
    "\n",
    "        # Save annotated_dataset image for visualization\n",
    "        ann = Annotator(add_object.copy())\n",
    "        ann.box_label(object_box, color=(255, 0, 0))\n",
    "        final = Image.fromarray(ann.result())\n",
    "        final.save(f\"{annotated_dir}/{i}.png\")\n",
    "\n",
    "        pbar.update(1)\n",
    "        pbar.refresh()\n",
    "        print(f\"Processed image {i}\")\n"
   ],
   "id": "23844e3812e79fd2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "failed = failed_removals + failed_adds\n",
    "failure_rate = failed / num_images\n",
    "successful = num_images - failed\n",
    "print(f\"Successful images: {successful}, Failed images: {failed}\")\n",
    "print(f\"Processing completed with failure rate: {failure_rate:.2%}\")\n",
    "print(f\"Failed person removals: {failed_removals}, Failed object additions: {failed_adds}\")"
   ],
   "id": "acfbc1d20ccd8081",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
